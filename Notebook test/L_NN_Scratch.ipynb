{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b8e14a-5756-4ef8-9cbb-bb7055d1a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73e5f61-836f-4f45-8464-065e5cc0565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e4bb18e-89a2-463b-8d65-6f589e482f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.69962487, -0.59315642,  0.60077125,  0.12684996],\n",
      "       [-0.50997171, -0.91073928, -0.50222596,  0.26526858],\n",
      "       [ 0.30496145, -0.52508338,  0.21650796, -0.59303325]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.48938113,  0.24792439,  0.01731284],\n",
      "       [-0.78811782, -0.46904407,  0.22939774],\n",
      "       [-0.388276  ,  0.50307799,  0.23461079]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[-0.49175147,  0.24178184,  0.49878077]]), 'b3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "test = initializeWeights([4,3,3,1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8ef3c5e-7f24-437c-b76d-70f7a78904b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a08a22-f87f-476c-9306-c3bc006efb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861b088f-552e-42aa-8012-7ebf9a9c845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7491d96a-c11a-4449-b523-2fbef5e6f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearForward(A, W, b):\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7320651-0770-4651-bc91-47a356dc7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardSoftmax(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = softmax(Z)\n",
    "    cache = (linearCache, activationCache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca78eca-604e-4060-9019-9eae1e9e75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardRelu(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = relu(Z)\n",
    "\n",
    "    cache = (linearCache, activationCache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79a1cbf8-5366-4ed9-956a-70ef67bba638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardSigmoid(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = sigmoid(Z)\n",
    "\n",
    "    cache = (linearCache, activationCache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7253488-7422-4a8a-b08b-a3324dad0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linearActivationForwardRelu(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    AL, cache = linearActivationForwardSigmoid(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[0]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e9c7e9-73fb-49e2-a8ba-8eb00c873848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropSoftmax(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linearActivationForwardRelu(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    AL, cache = linearActivationForwardSoftmax(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (10,X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1edd5424-03b2-43a5-833c-3c9b118cee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a01522f-4474-49d9-8499-258fccfbd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostSoftMax(AL, Y):\n",
    "    Y = one_hot(Y)\n",
    "    m = Y.shape[1]\n",
    "    #cost =  -np.sum(np.dot(one_hot(Y), np.log(AL + epsilon).T), axis=0, keepdims=True) / m\n",
    "    cost = -(1/m) * np.sum(Y * np.log(AL))\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7920bea3-697b-436e-8640-b43a665bbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "419a3fa1-a02e-4f74-8411-0da99d0e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dbd82cb-fca5-4006-aee8-41b31cd76428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache, Y):\n",
    "    Z = cache\n",
    "\n",
    "    sm, cache = softmax(Z)\n",
    "    \n",
    "    dZ = sm - Y\n",
    "    print(\"sm \" + str(sm.shape))\n",
    "    print(\"Y \" + str(Y.shape))\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37789d9c-5255-4be0-a239-98046405e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBackward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a9b7418-6060-494a-9781-19a1ea1bda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardRelu(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdb001db-9a4f-4c3d-9a11-cc062c38721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardSigmoid(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b00b419-9cf3-4362-b05f-451df2c0bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardSoftmax(dA, cache, Y):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = softmax_backward(dA, activation_cache, Y)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca1e63ad-a1f4-46fb-a5d0-cbc37a3fd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67816069-43fd-454d-9e8e-67dda205619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBackward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivationBackwardSigmoid(dAL, current_cache)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linearActivationBackwardRelu(grads[\"dA\" + str(l + 1)], current_cache)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9a2edf2-203b-4b75-aa9f-fa31f699a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBackwardSoftmax(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = one_hot(Y)\n",
    "    \n",
    "    dAL = AL - Y\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivationBackwardSoftmax(dAL, current_cache, Y)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linearActivationBackwardRelu(grads[\"dA\" + str(l + 1)], current_cache)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b272fe9f-6f2b-4206-8cc1-203d3ccee2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89feebee-dfee-4674-9085-c49c1ef7720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(X, Y, layerDims, epochs, learning_rate, print_cost=True):\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initializeWeights(layerDims)\n",
    "    \n",
    "    # Descente de gradient\n",
    "    for i in range(0, epochs):\n",
    "        AL, caches = forwardProp(X, parameters)\n",
    "        \n",
    "        cost = computeCost(AL, Y)\n",
    "\n",
    "        grads = modelBackward(AL, Y, caches)\n",
    "        \n",
    "        parameters = updateParameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == epochs:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef074068-1732-4a9c-9505-edfd654cc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test if softmax is working\n",
    "def modelFitSoftmax(X, Y, layerDims, epochs, learning_rate, print_cost=True):\n",
    "    costs = []\n",
    "    parameters = initializeWeights(layerDims)\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        AL, caches = forwardPropSoftmax(X, parameters)\n",
    "        \n",
    "        cost = computeCostSoftMax(AL, Y)\n",
    "\n",
    "        grads = modelBackwardSoftmax(AL, Y, caches)\n",
    "        \n",
    "        parameters = updateParameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            #print(\"iteration\" + str(i))\n",
    "        if i % 100 == 0 or i == epochs:\n",
    "            costs.append(cost)\n",
    "            #print(\"iteration\" + str(i))\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87e89b49-80c4-43fd-ba32-f37d038449cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forwardProp(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1f100a7-afc2-4a6c-a8bd-2618319e7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSoftmax(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forwardProp(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == one_hot(y))/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fed040ee-65d8-4af2-a96a-1ee7feb9c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbb8c23e-2bd0-47ea-9428-c78271c4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5be8a46-9000-4f60-a459-3f01b410baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f895393-2b6a-46d7-a376-0ca03cceeb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcec40e1-819d-4681-9f50-f70deab52127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bb878a3-cb6b-4e86-bb5e-3022a75d5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c468c165-fa2a-4b94-ae6e-8f0ff5c5757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6984292772096564\n",
      "Cost after iteration 100: 0.5857762212064846\n",
      "Cost after iteration 200: 0.48967800289004065\n",
      "Cost after iteration 300: 0.4307621075219954\n",
      "Cost after iteration 400: 0.3672587203098215\n",
      "Cost after iteration 500: 0.30322648732121754\n",
      "Cost after iteration 600: 0.28953211754066116\n",
      "Cost after iteration 700: 0.11449638430044717\n",
      "Cost after iteration 800: 0.06883145101862254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mmodelFit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.009\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mmodelFit\u001b[0;34m(X, Y, layerDims, epochs, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      8\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m forwardProp(X, parameters)\n\u001b[1;32m     10\u001b[0m cost \u001b[38;5;241m=\u001b[39m computeCost(AL, Y)\n\u001b[0;32m---> 12\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mmodelBackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m parameters \u001b[38;5;241m=\u001b[39m updateParameters(parameters, grads, learning_rate)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_cost \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36mmodelBackward\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     13\u001b[0m     current_cache \u001b[38;5;241m=\u001b[39m caches[l]\n\u001b[0;32m---> 14\u001b[0m     dA_prev_temp, dW_temp, db_temp \u001b[38;5;241m=\u001b[39m \u001b[43mlinearActivationBackwardRelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m=\u001b[39m dA_prev_temp\n\u001b[1;32m     16\u001b[0m     grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m dW_temp\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mlinearActivationBackwardRelu\u001b[0;34m(dA, cache)\u001b[0m\n\u001b[1;32m      2\u001b[0m linear_cache, activation_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m      3\u001b[0m dZ \u001b[38;5;241m=\u001b[39m relu_backward(dA, activation_cache)\n\u001b[0;32m----> 5\u001b[0m dA_prev, dW, db \u001b[38;5;241m=\u001b[39m \u001b[43mlinearBackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev, dW, db\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mlinearBackward\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m      5\u001b[0m dW \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dZ, A_prev\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      6\u001b[0m db \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m dA_prev \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev, dW, db\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = modelFit(train_x, train_y, [12288, 64, 32, 16, 1], 2500, 0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "297640b6-77b1-4d98-8c97-6d1b7356831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "[[1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      "  1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
      "  1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(test_x, test_y, param[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77240366-ccc2-4ca2-bb12-8927efcae154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 13:46:49.962349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee97d612-fb64-4d05-840f-50e141f70696",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x_mnist, train_y_mnist), (test_x_mnist, test_y_mnist) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26e033d6-ef06-462c-b961-ff3fcbdd59c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (28, 28, 3)\n",
      "train_x_orig shape: (60000, 28, 28)\n",
      "train_y shape: (60000,)\n",
      "test_x_orig shape: (10000, 28, 28)\n",
      "test_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_mnist.shape[0]\n",
    "num_px = train_x_mnist.shape[1]\n",
    "m_test = test_x_mnist.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_mnist.shape))\n",
    "print (\"train_y shape: \" + str(train_y_mnist.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_mnist.shape))\n",
    "print (\"test_y shape: \" + str(test_y_mnist.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ae27735-0ec4-4649-8931-2d2a1d383da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 60000)\n",
      "test_x's shape: (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten_mnist = train_x_mnist.reshape(train_x_mnist.shape[0], -1).T\n",
    "test_x_flatten_mnist = test_x_mnist.reshape(test_x_mnist.shape[0], -1).T\n",
    "\n",
    "train_x_m = train_x_flatten_mnist/255.\n",
    "test_x_m = test_x_flatten_mnist/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x_m.shape))\n",
    "print (\"test_x's shape: \" + str(test_x_m.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53ca1e93-ba99-4d7b-a066-098ac9f7efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_y_mnist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcce74df-9d00-42d4-803a-3670367e281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 2000)\n"
     ]
    }
   ],
   "source": [
    "train_x_m = train_x_m[:, :2000]\n",
    "train_y_mnist = train_y_mnist[:2000]\n",
    "print(train_x_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d266874-f027-4847-be3d-693bc1acc82b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm (10, 2000)\n",
      "Y (10, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/yy4d4r2s471cy5jjyhzsp6bh0000gn/T/ipykernel_33632/4039371470.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  A = np.exp(Z) / np.sum(np.exp(Z))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type numpy.ndarray which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mmodelFitSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.009\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mmodelFitSoftmax\u001b[0;34m(X, Y, layerDims, epochs, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      7\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m forwardPropSoftmax(X, parameters)\n\u001b[1;32m      9\u001b[0m cost \u001b[38;5;241m=\u001b[39m computeCostSoftMax(AL, Y)\n\u001b[0;32m---> 11\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mmodelBackwardSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m parameters \u001b[38;5;241m=\u001b[39m updateParameters(parameters, grads, learning_rate)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_cost \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m, in \u001b[0;36mmodelBackwardSoftmax\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m      7\u001b[0m dAL \u001b[38;5;241m=\u001b[39m AL \u001b[38;5;241m-\u001b[39m Y\n\u001b[1;32m      9\u001b[0m current_cache \u001b[38;5;241m=\u001b[39m caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)] \u001b[38;5;241m=\u001b[39m \u001b[43mlinearActivationBackwardSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     13\u001b[0m     current_cache \u001b[38;5;241m=\u001b[39m caches[l]\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mlinearActivationBackwardSoftmax\u001b[0;34m(dA, cache, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m linear_cache, activation_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m      4\u001b[0m dZ \u001b[38;5;241m=\u001b[39m softmax_backward(dA, activation_cache, Y)\n\u001b[0;32m----> 6\u001b[0m dA_prev, dW, db \u001b[38;5;241m=\u001b[39m \u001b[43mlinearBackwardSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev, dW, db\n",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m, in \u001b[0;36mlinearBackwardSoftmax\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m      2\u001b[0m Z \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the derivative of the softmax function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the gradient of the cost with respect to Z\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dZ_prev \u001b[38;5;241m=\u001b[39m dZ \u001b[38;5;241m*\u001b[39m softmax_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m softmax_output)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(Z):\n\u001b[0;32m----> 2\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(Z))\n\u001b[1;32m      3\u001b[0m     cache \u001b[38;5;241m=\u001b[39m Z\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A, cache\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type numpy.ndarray which has no callable exp method"
     ]
    }
   ],
   "source": [
    "param = modelFitSoftmax(train_x_m, train_y_mnist, [784, 64, 32, 16, 10], 2500, 0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba93e0e-e1bf-48c4-ad8e-fb9ec08548cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
