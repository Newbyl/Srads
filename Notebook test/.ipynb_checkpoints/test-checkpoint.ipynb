{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b8e14a-5756-4ef8-9cbb-bb7055d1a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73e5f61-836f-4f45-8464-065e5cc0565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4bb18e-89a2-463b-8d65-6f589e482f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.0669622 , -1.85977437, -0.26247356, -0.38301282],\n",
      "       [-1.31812665, -0.47335277,  0.55725299, -0.80418277],\n",
      "       [-0.32518027,  1.34822662, -0.17086152,  0.34097595]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.40333836,  1.12252526, -0.19206922],\n",
      "       [-0.2537357 ,  0.38399744, -0.46747021],\n",
      "       [-0.95361968,  0.99488607, -0.06935707]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[1.1287419 , 0.04830647, 0.13329647]]), 'b3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "test = initializeWeights([4,3,3,1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8ef3c5e-7f24-437c-b76d-70f7a78904b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3a08a22-f87f-476c-9306-c3bc006efb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "861b088f-552e-42aa-8012-7ebf9a9c845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7491d96a-c11a-4449-b523-2fbef5e6f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearForward(A, W, b):\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7320651-0770-4651-bc91-47a356dc7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardSoftmax(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = softmax(Z)\n",
    "\n",
    "    cache = (linearCache, activationCache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cca78eca-604e-4060-9019-9eae1e9e75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardRelu(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = relu(Z)\n",
    "\n",
    "    cache = (linearCache, activationCache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a1cbf8-5366-4ed9-956a-70ef67bba638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationForwardSigmoid(aPrev, W, b):\n",
    "    Z, linearCache = linearForward(aPrev, W, b)\n",
    "    A, activationCache = sigmoid(Z)\n",
    "\n",
    "    cache = (linearCache, activationCache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7253488-7422-4a8a-b08b-a3324dad0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linearActivationForwardRelu(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    AL, cache = linearActivationForwardSigmoid(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37e9c7e9-73fb-49e2-a8ba-8eb00c873848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropSoftmax(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linearActivationForwardRelu(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    AL, cache = linearActivationForwardSoftmax(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1edd5424-03b2-43a5-833c-3c9b118cee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a01522f-4474-49d9-8499-258fccfbd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostSoftMax(AL, Y, epsilon=1e-12):\n",
    "    m = Y.shape[0]\n",
    "    cost =  -np.sum(one_hot(Y) * np.log(AL + epsilon), axis=0, keepdims=True) / m\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7920bea3-697b-436e-8640-b43a665bbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "419a3fa1-a02e-4f74-8411-0da99d0e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dbd82cb-fca5-4006-aee8-41b31cd76428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    exp_Z = np.exp(Z)\n",
    "    softmax = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    dZ = softmax - dA\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37789d9c-5255-4be0-a239-98046405e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBackward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a9b7418-6060-494a-9781-19a1ea1bda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardRelu(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdb001db-9a4f-4c3d-9a11-cc062c38721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardSigmoid(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b00b419-9cf3-4362-b05f-451df2c0bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActivationBackwardSoftmax(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linearBackward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca1e63ad-a1f4-46fb-a5d0-cbc37a3fd4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "one_hot(np.array([0,1,3,4,0,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67816069-43fd-454d-9e8e-67dda205619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBackward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivationBackwardSigmoid(dAL, current_cache)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linearActivationBackwardRelu(grads[\"dA\" + str(l + 1)], current_cache)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9a2edf2-203b-4b75-aa9f-fa31f699a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBackwardSoftmax(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = one_hot(Y)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivationBackwardSoftmax(dAL, current_cache)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linearActivationBackwardRelu(grads[\"dA\" + str(l + 1)], current_cache)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b272fe9f-6f2b-4206-8cc1-203d3ccee2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89feebee-dfee-4674-9085-c49c1ef7720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(X, Y, layerDims, epochs, learning_rate, print_cost=True):\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initializeWeights(layerDims)\n",
    "    \n",
    "    # Descente de gradient\n",
    "    for i in range(0, epochs):\n",
    "        AL, caches = forwardProp(X, parameters)\n",
    "        \n",
    "        cost = computeCost(AL, Y)\n",
    "\n",
    "        grads = modelBackward(AL, Y, caches)\n",
    "        \n",
    "        parameters = updateParameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == epochs:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef074068-1732-4a9c-9505-edfd654cc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test if softmax is working\n",
    "def modelFitSoftmax(X, Y, layerDims, epochs, learning_rate, print_cost=True):\n",
    "    costs = []\n",
    "    parameters = initializeWeights(layerDims)\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        AL, caches = forwardPropSoftmax(X, parameters)\n",
    "        \n",
    "        #cost = computeCostSoftMax(AL, Y)\n",
    "\n",
    "        grads = modelBackwardSoftmax(AL, Y, caches)\n",
    "        \n",
    "        parameters = updateParameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            #print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            print(\"iteration\" + str(i))\n",
    "        if i % 100 == 0 or i == epochs:\n",
    "            #costs.append(cost)\n",
    "            print(\"iteration\" + str(i))\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87e89b49-80c4-43fd-ba32-f37d038449cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forwardProp(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1f100a7-afc2-4a6c-a8bd-2618319e7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSoftmax(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forwardProp(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == one_hot(y))/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fed040ee-65d8-4af2-a96a-1ee7feb9c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dbb8c23e-2bd0-47ea-9428-c78271c4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e5be8a46-9000-4f60-a459-3f01b410baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f895393-2b6a-46d7-a376-0ca03cceeb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dcec40e1-819d-4681-9f50-f70deab52127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1bb878a3-cb6b-4e86-bb5e-3022a75d5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c468c165-fa2a-4b94-ae6e-8f0ff5c5757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7212545152353563\n",
      "Cost after iteration 100: 0.594611996749886\n",
      "Cost after iteration 200: 0.5437067042597034\n",
      "Cost after iteration 300: 0.48470873199103964\n",
      "Cost after iteration 400: 0.4286362627808889\n",
      "Cost after iteration 500: 0.3683182218096129\n",
      "Cost after iteration 600: 0.2895424499085322\n",
      "Cost after iteration 700: 0.15229210718248334\n",
      "Cost after iteration 800: 0.09766842240340667\n",
      "Cost after iteration 900: 0.06417986282988597\n",
      "Cost after iteration 1000: 0.043013320966717734\n",
      "Cost after iteration 1100: 0.03064791562317098\n",
      "Cost after iteration 1200: 0.02173385426735686\n",
      "Cost after iteration 1300: 0.016599528332941798\n",
      "Cost after iteration 1400: 0.012499312215914531\n",
      "Cost after iteration 1500: 0.009780367679394903\n",
      "Cost after iteration 1600: 0.007912856562713087\n",
      "Cost after iteration 1700: 0.006569947698800652\n",
      "Cost after iteration 1800: 0.005504979404175554\n",
      "Cost after iteration 1900: 0.004707875541822656\n",
      "Cost after iteration 2000: 0.004093266632146202\n",
      "Cost after iteration 2100: 0.003604662633179828\n",
      "Cost after iteration 2200: 0.003207815046516103\n",
      "Cost after iteration 2300: 0.0028812447491387916\n",
      "Cost after iteration 2400: 0.002608505306220387\n",
      "Cost after iteration 2499: 0.002379415261084176\n"
     ]
    }
   ],
   "source": [
    "param = modelFit(train_x, train_y, [12288, 64, 32, 16, 1], 2500, 0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "297640b6-77b1-4d98-8c97-6d1b7356831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "[[1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      "  1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
      "  1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(test_x, test_y, param[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77240366-ccc2-4ca2-bb12-8927efcae154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:19:48.751941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee97d612-fb64-4d05-840f-50e141f70696",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x_mnist, train_y_mnist), (test_x_mnist, test_y_mnist) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26e033d6-ef06-462c-b961-ff3fcbdd59c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (28, 28, 3)\n",
      "train_x_orig shape: (60000, 28, 28)\n",
      "train_y shape: (60000,)\n",
      "test_x_orig shape: (10000, 28, 28)\n",
      "test_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_mnist.shape[0]\n",
    "num_px = train_x_mnist.shape[1]\n",
    "m_test = test_x_mnist.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_mnist.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_mnist.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ae27735-0ec4-4649-8931-2d2a1d383da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 60000)\n",
      "test_x's shape: (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten_mnist = train_x_mnist.reshape(train_x_mnist.shape[0], -1).T\n",
    "test_x_flatten_mnist = test_x_mnist.reshape(test_x_mnist.shape[0], -1).T\n",
    "\n",
    "train_x_m = train_x_flatten_mnist/255.\n",
    "test_x_m = test_x_flatten_mnist/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x_m.shape))\n",
    "print (\"test_x's shape: \" + str(test_x_m.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53ca1e93-ba99-4d7b-a066-098ac9f7efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_y_mnist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d266874-f027-4847-be3d-693bc1acc82b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 600000 into shape (9,60000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mmodelFitSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m, in \u001b[0;36mmodelFitSoftmax\u001b[0;34m(X, Y, layerDims, epochs, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      7\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m forwardPropSoftmax(X, parameters)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#cost = computeCostSoftMax(AL, Y)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mmodelBackwardSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m parameters \u001b[38;5;241m=\u001b[39m updateParameters(parameters, grads, learning_rate)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_cost \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m, in \u001b[0;36mmodelBackwardSoftmax\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m      4\u001b[0m m \u001b[38;5;241m=\u001b[39m AL\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m one_hot(Y)\n\u001b[0;32m----> 6\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m dAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mdivide(Y, AL) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m AL))\n\u001b[1;32m     11\u001b[0m current_cache \u001b[38;5;241m=\u001b[39m caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 600000 into shape (9,60000)"
     ]
    }
   ],
   "source": [
    "param = modelFitSoftmax(train_x_m, train_y_mnist, [784, 64, 32, 16, 9], 100, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7a990cd-8f2d-4742-be32-ac4580cdda49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m, in \u001b[0;36mpredictSoftmax\u001b[0;34m(X, y, parameters)\u001b[0m\n\u001b[1;32m      3\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[1;32m      4\u001b[0m p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,m))\n\u001b[0;32m----> 6\u001b[0m probas, caches \u001b[38;5;241m=\u001b[39m \u001b[43mforwardProp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, probas\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probas[\u001b[38;5;241m0\u001b[39m,i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mforwardProp\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m      9\u001b[0m     A, cache \u001b[38;5;241m=\u001b[39m linearActivationForwardRelu(A_prev, parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)], parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)])\n\u001b[1;32m     10\u001b[0m     caches\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[0;32m---> 13\u001b[0m AL, cache \u001b[38;5;241m=\u001b[39m linearActivationForwardSigmoid(A, \u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)])\n\u001b[1;32m     14\u001b[0m caches\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(AL\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "predictSoftmax(test_x_m, test_y_mnist, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ce0880f-9062-42f5-9935-f3586f6c3dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'W1': array([[        nan,         nan,         nan, ...,         nan,\n",
      "                nan,         nan],\n",
      "       [ 0.0303408 ,  0.02065285, -0.03173334, ..., -0.02938268,\n",
      "         0.02026407, -0.05340961],\n",
      "       [-0.03859692, -0.01792912,  0.00732674, ...,  0.03557969,\n",
      "        -0.03527355,  0.04556562],\n",
      "       ...,\n",
      "       [        nan,         nan,         nan, ...,         nan,\n",
      "                nan,         nan],\n",
      "       [        nan,         nan,         nan, ...,         nan,\n",
      "                nan,         nan],\n",
      "       [        nan,         nan,         nan, ...,         nan,\n",
      "                nan,         nan]]), 'b1': array([[         nan],\n",
      "       [ -4.07046516],\n",
      "       [ -3.45215502],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [-13.23024455],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [ -2.61482856],\n",
      "       [ -0.567255  ],\n",
      "       [ -0.39096978],\n",
      "       [         nan],\n",
      "       [ -1.64903817],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [ -3.03575531],\n",
      "       [         nan],\n",
      "       [ -5.61583277],\n",
      "       [-23.23660432],\n",
      "       [-10.02822439],\n",
      "       [ -4.88992146],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [ -9.66119766],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [-11.69268293],\n",
      "       [ -0.81868536],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [ -8.04192825],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [ -4.46788609],\n",
      "       [ -6.05234946],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan],\n",
      "       [         nan]]), 'W2': array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]]), 'b2': array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]]), 'W3': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan]]), 'b3': array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]]), 'W4': array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan],\n",
      "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan]]), 'b4': array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]])}, [])\n"
     ]
    }
   ],
   "source": [
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba93e0e-e1bf-48c4-ad8e-fb9ec08548cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
